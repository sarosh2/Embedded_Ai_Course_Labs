{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSkGdV3Kj1jl"
      },
      "outputs": [],
      "source": [
        " pip install -q tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "lnT1CSPYj5sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train[:90%]', 'train[90%:]', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")"
      ],
      "metadata": {
        "id": "KJeZXywXnTn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_img(image:tf.uint8, label:tf.int64):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "def normalize_splits(ds, split_name: str, batch_size: int):\n",
        "  \"\"\"Applies preprocessing to train, val and test sets\"\"\"\n",
        "  ds = ds.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.cache() # Caching makes it faster for consecutive runs\n",
        "  if split_name != 'test':\n",
        "    # Shuffling is not done for the test set\n",
        "    ds = ds.shuffle(ds_info.splits[split_name].num_examples)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  return ds"
      ],
      "metadata": {
        "id": "_gVNOA9mnkGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('/content/Full_Precision_MNIST_TF.h5')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SKyBUnU5kSAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb5cee5-a4b5-4bac-8ea0-5cfae7334960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 14, 14, 6)         0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 16)        2416      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 16)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 120)               94200     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 107786 (421.04 KB)\n",
            "Trainable params: 107786 (421.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set.\n",
        "\n",
        "num_images = 60000 * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define model for pruning.\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXnuTNoDkACh",
        "outputId": "d6444099-abc2-4163-dbc5-cbf523f1edc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_conv2d  (None, 28, 28, 6)         308       \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_max_po  (None, 14, 14, 6)         1         \n",
            " oling2d (PruneLowMagnitude                                      \n",
            " )                                                               \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d  (None, 14, 14, 16)        4818      \n",
            " _1 (PruneLowMagnitude)                                          \n",
            "                                                                 \n",
            " prune_low_magnitude_max_po  (None, 7, 7, 16)          1         \n",
            " oling2d_1 (PruneLowMagnitu                                      \n",
            " de)                                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_flatte  (None, 784)               1         \n",
            " n (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_dense   (None, 120)               188282    \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_dense_  (None, 84)                20246     \n",
            " 1 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_dense_  (None, 10)                1692      \n",
            " 2 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 215349 (841.24 KB)\n",
            "Trainable params: 107786 (421.04 KB)\n",
            "Non-trainable params: 107563 (420.20 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = normalize_splits(ds_train, split_name='train[:90%]', batch_size=batch_size)\n",
        "ds_val = normalize_splits(ds_val, split_name='train[90%:]', batch_size=batch_size)\n",
        "ds_test = normalize_splits(ds_test, split_name='test', batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Wn8tdi2Lnlft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "model_for_pruning.fit(ds_train, validation_data=ds_val, epochs=epochs, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "AEtkDQ7cm_OP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6be341a3-d20c-4033-9f78-373358e28171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "422/422 [==============================] - 73s 79ms/step - loss: 0.0657 - accuracy: 0.9805 - val_loss: 0.0914 - val_accuracy: 0.9750\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 35s 83ms/step - loss: 0.0559 - accuracy: 0.9837 - val_loss: 0.0572 - val_accuracy: 0.9830\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7e02e1e9fee0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, baseline_accuracy = model.evaluate(ds_test, verbose=0)\n",
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "   ds_test, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_accuracy)\n",
        "print('Pruned test accuracy:', model_for_pruning_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXtaHf0foF_d",
        "outputId": "a235eec9-2b1e-4f0c-dbd4-70e0f70a2dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy: 0.9854999780654907\n",
            "Pruned test accuracy: 0.9854999780654907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "pruned_keras_file = \"pruned_model.h5\"\n",
        "keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ1Fp1P_q77m",
        "outputId": "a3e14ac3-321f-423e-93ad-f30eec87625b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-c87644c9ed40>:4: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned Keras model to: pruned_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "pruned_tflite_file = 'pruned_tflite_file.tflite'\n",
        "\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)\n",
        "\n",
        "print('Saved pruned TFLite model to:', pruned_tflite_file)"
      ],
      "metadata": {
        "id": "ZFxSLJMKrdsV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6f0152-0432-48cf-bc4e-2d1d29755cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned TFLite model to: pruned_tflite_file.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "RcMuqOlVtP4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size('Full_Precision_MNIST_TF.h5')))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiLC37NdtXu6",
        "outputId": "4019bfe0-2b5d-4b14-9d7f-6448b27576fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of gzipped baseline Keras model: 1200897.00 bytes\n",
            "Size of gzipped pruned Keras model: 131382.00 bytes\n",
            "Size of gzipped pruned TFlite model: 126220.00 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def representative_dataset():\n",
        "    for images, _ in ds_train.take(100):\n",
        "        yield [tf.cast(images, tf.float32)]"
      ],
      "metadata": {
        "id": "KOT1f72rksO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "converter.representative_dataset = representative_dataset\n",
        "quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "quantized_and_pruned_tflite_file = 'quantized_and_pruned_tflite_file.tflite'\n",
        "\n",
        "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
        "  f.write(quantized_and_pruned_tflite_model)\n",
        "\n",
        "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
        "\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size('Full_Precision_MNIST_TF.h5')))\n",
        "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57DOiE9FtnAu",
        "outputId": "2107bedf-e29c-45ad-d04c-2ce9431f2775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved quantized and pruned TFLite model to: quantized_and_pruned_tflite_file.tflite\n",
            "Size of gzipped baseline Keras model: 1200897.00 bytes\n",
            "Size of gzipped pruned and quantized TFlite model: 39008.00 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "test_images = []\n",
        "test_labels = []\n",
        "\n",
        "# Iterate over the test dataset\n",
        "for images, labels in ds_test:\n",
        "    test_images.append(images.numpy())  # Converting tensor to numpy array\n",
        "    test_labels.append(labels.numpy())\n",
        "\n",
        "# Concatenate all batches to form a single array\n",
        "test_images = np.concatenate(test_images, axis=0)\n",
        "test_labels = np.concatenate(test_labels, axis=0)"
      ],
      "metadata": {
        "id": "Q_Jc-FghxwBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_images(images):\n",
        "    # Scale from [0, 1] float32 to [0, 255] uint8, then to [-128, 127] int8\n",
        "    images = (images * 255.0).astype(np.uint8)  # Scale to [0, 255]\n",
        "    return images\n",
        "\n",
        "test_images_int8 = quantize_images(test_images)"
      ],
      "metadata": {
        "id": "EubRFOL9lkw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(interpreter):\n",
        "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_image in enumerate(test_images_int8):\n",
        "        if i % 1000 == 0:\n",
        "            print(f'Evaluated on {i} results so far.')\n",
        "\n",
        "        # Pre-processing: add batch dimension\n",
        "        test_image = np.expand_dims(test_image, axis=0)\n",
        "        interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "        # Run inference.\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Post-processing: remove batch dimension and find the digit with highest probability.\n",
        "        output = interpreter.get_tensor(output_index)\n",
        "        digit = np.argmax(output[0])\n",
        "        prediction_digits.append(digit)\n",
        "\n",
        "    print('\\n')\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "ffXkVmyjuZf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
        "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJYbnZzsu3CZ",
        "outputId": "1d41afb6-0965-47b9-c4b4-87d3597f68dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "Evaluated on 7000 results so far.\n",
            "Evaluated on 8000 results so far.\n",
            "Evaluated on 9000 results so far.\n",
            "\n",
            "\n",
            "Pruned and quantized TFLite test_accuracy: 0.9854\n",
            "Pruned TF test accuracy: 0.9854999780654907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat quantized_and_pruned_tflite_file.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOGyKkdomWb3",
        "outputId": "4f146127-a6ec-4d7f-935a-57ec3a13cff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Header file, model.h, is 698,546 bytes.\n",
            "\n",
            "Open the side panel (refresh if needed). Double click model.h to download the file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbM_y4O7nSkN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}